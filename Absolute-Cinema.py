import os
import cv2
import numpy as np
import tempfile
import logging
import asyncio
import random
import gc
from uuid import uuid4
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters
from moviepy.editor import VideoFileClip, AudioFileClip

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MAX_FILE_SIZE = 20 * 1024 * 1024  # 20MB
TEMP_DIR = tempfile.gettempdir()
user_sessions = {}
REQUEST_TIMEOUT = 30  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–æ 30 —Å–µ–∫—É–Ω–¥

def validate_shapes(img1: np.ndarray, img2: np.ndarray):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞–Ω–∞–ª–æ–≤"""
    if img1.shape != img2.shape:
        raise ValueError(f"Shape mismatch: {img1.shape} vs {img2.shape}")

def create_scratch_texture(height: int, width: int) -> np.ndarray:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç—É—Ä—ã —Ü–∞—Ä–∞–ø–∏–Ω"""
    texture = np.zeros((height, width, 3), dtype=np.uint8)
    for _ in range(random.randint(2, 5)):
        y = random.randint(0, height)
        thickness = random.randint(1, 3)
        cv2.line(texture, (0, y), (width, y), (200, 200, 200), thickness)
    return texture

def apply_old_film(frame: np.ndarray) -> np.ndarray:
    """–≠—Ñ—Ñ–µ–∫—Ç —Å—Ç–∞—Ä–æ–π –∫–∏–Ω–æ–ø–ª–µ–Ω–∫–∏"""
    h, w, c = frame.shape
    
    # –°–µ–ø–∏—è
    sepia_kernel = np.array([
        [0.272, 0.534, 0.131],
        [0.349, 0.686, 0.168],
        [0.393, 0.769, 0.189]
    ], dtype=np.float32)
    
    processed = cv2.transform(frame.astype(np.float32), sepia_kernel)
    processed = np.clip(processed, 0, 255).astype(np.uint8)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞
    noise = np.random.normal(0, 15, (h, w, c)).astype(np.uint8)
    validate_shapes(processed, noise)
    processed = cv2.add(processed, noise)

    # –°–ª—É—á–∞–π–Ω—ã–µ —Ü–∞—Ä–∞–ø–∏–Ω—ã
    if random.random() < 0.3:
        texture = create_scratch_texture(h, w)
        if texture.shape != processed.shape:
            texture = cv2.resize(texture, (w, h))
        validate_shapes(processed, texture)
        processed = cv2.addWeighted(processed, 0.9, texture, 0.1, 0)

    # –í–∏–Ω—å–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    kernel_x = cv2.getGaussianKernel(w, w//3)
    kernel_y = cv2.getGaussianKernel(h, h//3)
    kernel = np.outer(kernel_y, kernel_x)
    mask = (kernel * 0.7 + 0.3)[:, :, np.newaxis]
    mask = np.repeat(mask, c, axis=2).astype(np.float32)
    
    validate_shapes(processed, mask)
    return cv2.multiply(processed.astype(np.float32), mask, dtype=cv2.CV_8UC3)

def apply_brightness(frame: np.ndarray) -> np.ndarray:
    """–ü–æ–≤—ã—à–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —ç—Ñ—Ñ–µ–∫—Ç–æ–º"""
    return cv2.convertScaleAbs(frame, alpha=1.2, beta=50)  # –£–≤–µ–ª–∏—á–µ–Ω—ã –æ–±–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞

def apply_negative(frame: np.ndarray) -> np.ndarray:
    """–ù–µ–≥–∞—Ç–∏–≤"""
    return cv2.bitwise_not(frame)

def apply_pixelation(frame: np.ndarray) -> np.ndarray:
    """–ü–∏–∫—Å–µ–ª–∏–∑–∞—Ü–∏—è"""
    h, w = frame.shape[:2]
    new_w = max(1, w // 10)
    new_h = max(1, h // 10)
    small = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
    return cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)

def apply_edge_detection(frame: np.ndarray) -> np.ndarray:
    """–í—ã–¥–µ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    return cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)

def apply_vignette(frame: np.ndarray) -> np.ndarray:
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –≤–∏–Ω—å–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –º–∞—Å–∫–∏"""
    h, w = frame.shape[:2]
    
    # –°–æ–∑–¥–∞—ë–º –º–∞—Å–∫—É —Å —Ç—Ä–µ–º—è –∫–∞–Ω–∞–ª–∞–º–∏
    x = np.linspace(-1, 1, w)
    y = np.linspace(-1, 1, h)
    X, Y = np.meshgrid(x, y)
    R = np.sqrt(X**2 + Y**2)
    
    # –°–æ–∑–¥–∞—ë–º 3-–∫–∞–Ω–∞–ª—å–Ω—É—é –º–∞—Å–∫—É
    mask = 1 - np.clip(R / 0.8, 0, 1)
    mask = (mask * 0.7 + 0.3)
    mask = np.repeat(mask[..., np.newaxis], 3, axis=2)  # –î–æ–±–∞–≤–ª—è–µ–º 3 –∫–∞–Ω–∞–ª–∞

    # –£–±–µ–¥–∏–º—Å—è –≤ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
    if mask.shape != frame.shape:
        mask = cv2.resize(mask, (w, h))

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º
    return cv2.multiply(frame, mask.astype(np.float32), dtype=cv2.CV_8UC3)

def process_video_with_audio(input_path: str, output_path: str, effect_func):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ"""
    with VideoFileClip(input_path) as video_clip:
        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        video_clip = video_clip.resize(height=480)
        
        audio = video_clip.audio
        processed_frames = []
        
        for frame in video_clip.iter_frames():
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            processed = effect_func(frame_bgr)
            processed_rgb = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)
            processed_frames.append(processed_rgb)
        
        processed_clip = video_clip.set_make_frame(
            lambda t: processed_frames[min(int(t * video_clip.fps), len(processed_frames)-1)]
        )
        
        if audio:
            processed_clip = processed_clip.set_audio(audio)
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
        processed_clip.write_videofile(
            output_path,
            codec='libx264',
            audio_codec='aac',
            preset='fast',  # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            bitrate='500k', # –ö–æ–Ω—Ç—Ä–æ–ª—å –±–∏—Ç—Ä–µ–π—Ç–∞
            logger=None,
            threads=4
        )
        processed_clip.close()

async def apply_effect(update: Update, context: ContextTypes.DEFAULT_TYPE, effect_func, effect_name: str):
    user_id = update.message.from_user.id
    output_path = None
    attempts = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ—Ç–ø—Ä–∞–≤–∫–∏
    
    try:
        if user_id not in user_sessions or not os.path.exists(user_sessions[user_id]['input_path']):
            await update.message.reply_text("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–∏–¥–µ–æ!")
            return

        session = user_sessions[user_id]
        output_path = os.path.join(TEMP_DIR, f"processed_{session['file_id']}.mp4")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ
        await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: process_video_with_audio(session['input_path'], output_path, effect_func)
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        if os.path.getsize(output_path) > MAX_FILE_SIZE:
            await update.message.reply_text("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏!")
            return

        # –û—Ç–ø—Ä–∞–≤–∫–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for attempt in range(attempts):
            try:
                await context.bot.send_video(
                    chat_id=update.message.chat_id,
                    video=output_path,
                    caption=f"‚úÖ {effect_name}",
                    write_timeout=REQUEST_TIMEOUT,
                    connect_timeout=REQUEST_TIMEOUT
                )
                break
            except Exception as send_error:
                if attempt == attempts - 1:
                    raise send_error
                await asyncio.sleep(2)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        await update.message.reply_text("‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤–∏–¥–µ–æ")
        
    finally:
        # –û—á–∏—Å—Ç–∫–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for path in [output_path, user_sessions.get(user_id, {}).get('input_path')]:
            if path and os.path.exists(path):
                for _ in range(3):
                    try:
                        os.remove(path)
                        break
                    except:
                        await asyncio.sleep(0.5)
        
        if user_id in user_sessions:
            del user_sessions[user_id]
        
        gc.collect()

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    await update.message.reply_text(
        "üìπ –û—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–∏–¥–µ–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º–∞–∫—Å–∏–º—É–º 20MB)\n"
        "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ:\n"
        "/old_film - –≠—Ñ—Ñ–µ–∫—Ç —Å—Ç–∞—Ä–æ–π –ø–ª–µ–Ω–∫–∏\n"
        "/brightness - –ü–æ–≤—ã—à–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏\n"
        "/negative - –ù–µ–≥–∞—Ç–∏–≤\n"
        "/pixelate - –ü–∏–∫—Å–µ–ª–∏–∑–∞—Ü–∏—è\n"
        "/edges - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü\n"
        "/vignette - –í–∏–Ω—å–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"
    )

async def handle_video(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ"""
    try:
        user_id = update.message.from_user.id
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        if update.message.video.file_size > MAX_FILE_SIZE:
            await update.message.reply_text("‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π! –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 20MB")
            return

        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ
        video = update.message.video
        file = await video.get_file()
        file_id = str(uuid4())
        input_path = os.path.join(TEMP_DIR, f"{user_id}_{file_id}.mp4")
        
        await file.download_to_drive(input_path)
        user_sessions[user_id] = {
            'input_path': input_path,
            'file_id': file_id
        }

        await update.message.reply_text(
            "‚úÖ –í–∏–¥–µ–æ –ø–æ–ª—É—á–µ–Ω–æ! –í—ã–±–µ—Ä–∏—Ç–µ —ç—Ñ—Ñ–µ–∫—Ç:\n"
            "/old_film /brightness /negative /pixelate /edges /vignette"
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ: {e}", exc_info=True)
        await update.message.reply_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–∏–¥–µ–æ")

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∫–æ–º–∞–Ω–¥
async def old_film(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await apply_effect(update, context, apply_old_film, "–°—Ç–∞—Ä–∞—è –ø–ª–µ–Ω–∫–∞")

async def brightness(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await apply_effect(update, context, apply_brightness, "–ü–æ–≤—ã—à–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏")

async def negative(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await apply_effect(update, context, apply_negative, "–ù–µ–≥–∞—Ç–∏–≤")

async def pixelate(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await apply_effect(update, context, apply_pixelation, "–ü–∏–∫—Å–µ–ª–∏–∑–∞—Ü–∏—è")

async def edges(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await apply_effect(update, context, apply_edge_detection, "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü")

async def vignette(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await apply_effect(update, context, apply_vignette, "–í–∏–Ω—å–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ")

def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    application = Application.builder().token("7674173232:AAFBkten7YJZOh2sHwhvyKqdzuTAnrGfwJs").build()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.VIDEO, handle_video))
    application.add_handler(CommandHandler("old_film", old_film))
    application.add_handler(CommandHandler("brightness", brightness))
    application.add_handler(CommandHandler("negative", negative))
    application.add_handler(CommandHandler("pixelate", pixelate))
    application.add_handler(CommandHandler("edges", edges))
    application.add_handler(CommandHandler("vignette", vignette))
    
    application.run_polling()

if __name__ == "__main__":
    main()
